{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Robot Learning\n",
    "\n",
    "### Assignment 5\n",
    "\n",
    "Solutions are due on 17.05.2022 before the lecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "Please carefully read the following description of the card game *Blackjack*.\n",
    "\n",
    "The goal of Blackjack is to draw cards such that their values sum to as close to 21 as possible, but do not exceed 21. All cards 2 through 10 are assigned their number as the respective value, face cards like Jack, Queen and King yield a value of 10 points. The aces are special; they can count as either 1 or 11, depending on what is more beneficial. When using the ace as 11 points does not cause the total value of the hand to exceed 21, the ace is called 'usable'. \n",
    "\n",
    "The actions are to *stick*, which means to stop drawing cards and let the dealer play, or to *hit*, which means to draw another card to add to your hand. \n",
    "\n",
    "The opponent is a dealer with a fixed strategy; the dealer will always hit when the dealer's sum of card values is below 17 points. At or above 17 points, the dealer immediately sticks, irrespective of the player's card sum. Note that the dealer only starts playing once the player is finished, so when the player has selected to stick.\n",
    "\n",
    "The game starts by drawing two cards each to player and dealer from an infinite deck. The player can see the own cards, but only the first card which the dealer recieves, the other is face-down. If the player hits and exceeds 21 points, the player is considered *bust* and immediately loses the game. When the player decides to stick, the dealer will draw cards according to its strategy. If the dealer goes *bust* in the process, the player immediately wins the game. If not, the sum of the dealer's card values is compared to the sum of the player's card values. If they are equal, the game is considered a draw. If the player has a higher score, the player wins, and vice versa.\n",
    "\n",
    "In this RL setting, the rewards are +1 for winning, -1 for losing, and 0 for drawing and all intermediate moves. The state is a 3-tuple consisting of the players current hand sum, the value of the dealer's initial open card and a boolean describing whether the player currently has a usable ace or not. The action space consists of the integer $0$, which corresponds to *stick*, and the integer $1$, which corresponds to *hit*. Note that this corresponds exactly to the scenario described in [Example 5.1 in Reinforcement Learning: An Introduction by Sutton and Barto.](http://incompleteideas.net/book/the-book-2nd.html)\n",
    "\n",
    "To get started, please examine the following code cell, which loads the Blackjack environment from the utils file and plays an episode. The player is in this case the random agent. The environment is equivalent to the one provided in OpenAI Gym; we adapted the source code so that installing this python package is not necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-09T17:28:12.708438Z",
     "start_time": "2022-05-09T17:28:11.798886Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action Sequence: [0]\n",
      "Obtained Rewards: [-1.0]\n",
      "State Transitions: [(12, 1, 0), (12, 1, 0)]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from helpers.utils import BlackjackEnv\n",
    "\n",
    "np.random.seed(2022)\n",
    "# create a Blackjack environment\n",
    "casino = BlackjackEnv()\n",
    "\n",
    "# do this before each new episode\n",
    "# state is 3-tuple: (player_current_sum, dealer_first_open_card, usable_ace_bool)\n",
    "# if dealer_first_open_card == 1, the dealer is holding an ace!\n",
    "starting_state = casino.reset()\n",
    "\n",
    "# create flag tracking whether the episode is over\n",
    "done = False\n",
    "\n",
    "# create lists to save the trajectory and the decisions of the agent\n",
    "actions, rewards, states = [], [], [starting_state]\n",
    "\n",
    "# simulate until the episode is over\n",
    "while not done:\n",
    "    #in each non-terminal state the agent can either stick (0) or hit (1)\n",
    "    possibleActions = casino.getAvailableActions()\n",
    "    # select one of them randomly\n",
    "    a = np.random.choice(possibleActions)\n",
    "    # save the action\n",
    "    actions.append(a)\n",
    "    # execute the action using the step function\n",
    "    # s is the new state\n",
    "    # done is the flag whether the episode is over\n",
    "    # the fourth output _ is a dict containing additional information on the environment\n",
    "    # it is empty here, but passing it is according to convention in OpenAI Gym environments\n",
    "    s, r, done, _ = casino.step(action = a)\n",
    "    # save the states\n",
    "    rewards.append(r)\n",
    "    states.append(s)\n",
    "    \n",
    "print('Action Sequence:', actions)\n",
    "print('Obtained Rewards:', rewards)\n",
    "print('State Transitions:', states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5.1)\n",
    "\n",
    "Implement Sarsa($\\lambda$) for Blackjack. Use an $\\varepsilon$-greedy exploration strategy with $\\varepsilon = 0.05$ and a time-varying scalar step-size of $\\alpha_t = \\frac{1}{N(s_t,a_t)+1}$, where $N(s, a)$ is the number of times the value of this state-action pair has been updated.\n",
    "\n",
    "Run the algorithm with parameter values $\\lambda \\in \\{0, 0.1, 0.2, \\dots, 1\\}$. Stop exploration and learning\n",
    "after $100000$ episodes and plot the average return for the next $10000$ episodes against $\\lambda$. \n",
    "\n",
    "Note that there is a loop over all state-actions pairs in the Sarsa($\\lambda$) algorithm. This loop is straightforward (but also necessary) to vectorize. Do not hesitate to contact your tutors if you need help with this step. Also, the random agent scores around $-0.4$ on average, and your trained agents should be able to outperform this weak baseline.\n",
    "\n",
    "<div style=\"text-align: right; font-weight:bold\"> 5 Points </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-09T17:28:12.711333Z",
     "start_time": "2022-05-09T17:28:12.709741Z"
    }
   },
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5.2)\n",
    "\n",
    "Use your experiment from task 5.1) to select your favorite value for $\\lambda$. Learn $Q$ using the Sarsa($\\lambda$) algorithm for $1$ million episodes. After training, stop exploration and learning and report the average return for the next $10000$ episodes. Finally, visualize the resulting policy by using the helper function *plotPolicy(Q)*.\n",
    "\n",
    "<div style=\"text-align: right; font-weight:bold\"> 2 Points </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-09T17:28:12.713433Z",
     "start_time": "2022-05-09T17:28:12.712034Z"
    }
   },
   "outputs": [],
   "source": [
    "from helpers.utils import plotPolicy\n",
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5.3)\n",
    "\n",
    "Now consider a simple coarse coding value function approximator that is based on a binary feature vector $\\phi_{s, a} \\in \\mathbb{R}^{128}$ with $8 \\times 4 \\times 2 \\times 2 = 128$ features. Each binary feature has a value of 1 iff $(s, a)$ lies within the cuboid of state-action-space corresponding to that feature. The cuboids have the following overlapping intervals:\n",
    "\n",
    "$player(s) = \\{[4, 7], [6, 9], [8, 11], [10, 13], [12, 15], [14, 17], [18, 20], [19, 21]\\}$\n",
    "\n",
    "$dealer(s) = \\{[1,4], [3, 6], [5, 8], [7, 10]\\}$\n",
    "\n",
    "$usable\\_ace(s) = \\{0,1\\}$\n",
    "\n",
    "$a = \\{0, 1\\}$\n",
    "\n",
    "where\n",
    "\n",
    "* player(s) is the sum of the player’s cards (4–21) in state s\n",
    "* dealer(s) is the value of the dealer’s first card (1–10) in state s\n",
    "\n",
    "Implement a function $phi(s,a)$ which takes a state-action pair $(s,a)$ and maps it to a vector $\\phi_{s,a}$ as described above.\n",
    "\n",
    "<div style=\"text-align: right; font-weight:bold\"> 3 Points </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-09T17:28:12.716010Z",
     "start_time": "2022-05-09T17:28:12.714737Z"
    }
   },
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5.4)\n",
    "\n",
    "Conduct one more Sarsa($\\lambda$) experiment, but this time use a linear value function approximation $Q(s,a) = \\theta \\cdot \\phi_{s,a}$. Use a constant exploration of $\\varepsilon=0.05$, a constant step-size of $\\alpha=0.02$, and your favorite $\\lambda$.\n",
    "\n",
    "Stop exploration and learning after $50000$ episodes and report the average return for the next $10000$ episodes.\n",
    "\n",
    "To clarify, $\\theta$ is a row vector which your algorithm learns instead of $Q(s,a)$. The row vector times column vector product $Q(s,a) = \\theta \\cdot \\phi_{s,a}$ can also be understood as $Q(s,a) = \\sum_{i=0}^{127} \\theta(i)\\phi_{s,a}(i)$.\n",
    "\n",
    "<div style=\"text-align: right; font-weight:bold\"> 4 Points </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-09T17:28:12.718550Z",
     "start_time": "2022-05-09T17:28:12.716829Z"
    }
   },
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5.5)\n",
    "\n",
    "Implement the actor-critic version of TD(0) control. You can proceed as described on slide 4 in lecture 05. However, please adjust the update of the preferences $p(s,a)$ to\n",
    "\n",
    "$$p[s_t][a_t] \\leftarrow p[s_t][a_t] + \\beta \\delta_t [1 - \\pi(s,a_t)]$$\n",
    "\n",
    "This represents the learning rule for the actor. For the critic, you can update state values $V(s_t)$ according to\n",
    "\n",
    "$$V(s_t) \\leftarrow V(s_t) + \\alpha \\delta_t$$ \n",
    "\n",
    "after updating the actor for each step of the episode. Use $\\alpha = \\beta = 0.2$ and $\\gamma = 0.9$ and learn for 1 million episodes. Report the mean return of the final $10000$ episodes.\n",
    "\n",
    "Then, visualize the policy induced by $p$. It is possible to use *plotPolicy($\\pi$)* for this purpose.\n",
    "\n",
    "<div style=\"text-align: right; font-weight:bold\"> 6 Points </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-09T17:28:12.723164Z",
     "start_time": "2022-05-09T17:28:12.720288Z"
    }
   },
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
