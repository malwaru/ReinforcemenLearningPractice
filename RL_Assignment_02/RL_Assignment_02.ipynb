{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Robot Learning\n",
    "\n",
    "### Assignment 2\n",
    "\n",
    "Solutions are due on 26.04.2022 before the lecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "Consider the following $9 \\times 9$ grid world:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"helpers/gridworld_sketch_legend.png\" alt=\"Grid World\" title=\"Grid World\" width=\"650\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The agent may start in any cell that is not an obstacle nor the goal.\n",
    "\n",
    "It can choose between eight actions, which correspond to moving to the directions \n",
    "\n",
    "$$a_i \\in \\{NW,      N,      NE,     E,     SE,    S,     SW,     W\\}$$\n",
    "\n",
    "These are indexed according to the order above, i.e. $a_0 = NW$ and $a_6 = SW$.\n",
    "\n",
    "The agent must be careful, for the actions are non-deterministic! The agent moves with probability $0.7$ into the desired\n",
    "direction, but with probability $0.2$ deviates $45^{\\circ}$ to the left and with probability $0.1$ deviates $45^{\\circ}$ \n",
    "to the right of the desired direction due to treacherous gusts unexpectedly sweeping the grid.\n",
    "\n",
    "The rewards are structured as follows:\n",
    "\n",
    "* When it reaches a blue cell, it receives a little snack of 15 points.\n",
    "\n",
    "* When it attempts to enter a red obstacle cell, it receives -30 points and stays in the cell it came from.\n",
    "\n",
    "* When it attempts to leave the grid, it receives -30 points and stays in the cell it came from.\n",
    "\n",
    "* When the agent reaches the green goal cell, it receives 150 points and the episode ends.\n",
    "\n",
    "* All other actions entering a white cell receive -1 point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2.1\n",
    "\n",
    "To familiarize yourself with the environment above, answer the following questions:\n",
    "\n",
    "* The agent is at $s = (y_s, x_s) = (3, 5)$ and wants to execute $a_5$. What is the probability $P^a_{s,s'}$ for $s' =(4,6)$?\n",
    "\n",
    "\n",
    "* The agent is at $s = (3, 7)$ and wants to execute $a_3$. What is the expected value of the reward?\n",
    "\n",
    "<div style=\"text-align: right; font-weight:bold\"> 1 + 2 = 3 Points </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please answer in this text cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2.2)\n",
    "\n",
    "Using the *Iterative Policy Evaluation* Algorithm, compute the value $V^{\\pi}(s)$ of all accessible cells $s$ for a policy $\\pi(s,a)$ that chooses with probability $0.5$ a random action and otherwise attempts to move to the right.\n",
    "\n",
    "Intialize $V(s)$ with zero, use a discount parameter of $\\gamma=0.9$ and show your results by printing your state values $V^{\\pi}(s)$.\n",
    "\n",
    "<div style=\"text-align: right; font-weight:bold\"> 5 Points </div>\n",
    "\n",
    "#### Note\n",
    "\n",
    "For your convenience, you are provided the helper function *getNextStatesRewardsAndProbabilities(state, action)* which returns for a given state $s$ and an action $a$ a list of 3 -tuples of the form\n",
    "\n",
    "$$[(s_0', R^a_{s,s_0'}, P^a_{s,s_0'}), (s_1', R^a_{s,s_1'}, P^a_{s,s_1'}), \\dots]$$\n",
    "\n",
    "where $s_i'$ are all future states with $P^a_{s,s_i'} \\neq 0$. Here $s = (y, x)$ and $s_i' = (y_i', x_i')$ are both tuples of integers, $a \\in {0, \\dots, 7}$ is an integer, and $R^a_{s,s_i'}$, $P^a_{s,s_i'}$ are both floats.\n",
    "\n",
    "Also, please find below some data structures which you might find helpful. Create code and text cells as necessary to present your solution!\n",
    "\n",
    "In your implementation, $V(s)$ should be a $9 \\times 9$ numpy array and $\\pi(s,a)$ should be a $9 \\times 9 \\times 8$ numpy array, where $\\sum_a \\pi(s,a) = 1$ for all s!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sPrime: (0, 6) R: -30.0 P: 0.2\n",
      "sPrime: (0, 7) R: 15.0 P: 0.7\n",
      "sPrime: (0, 6) R: -30.0 P: 0.1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from helpers.utils import getNextStatesRewardsAndProbabilities\n",
    "%matplotlib inline\n",
    "\n",
    "#this is a list of all states\n",
    "states = [(y,x) for y in range(9) for x in range(9)]\n",
    "#this is a list of all states containing obstacles\n",
    "obstacles = [(1,5), (1,7), (2,1), (2,2), (2,3), (2,4), (2,5), (2,7),\\\n",
    "             (3,1), (3,6), (4,3), (4,4), (4,5), (5,7), \\\n",
    "             (6,1), (6,2), (6,3), (6,4), (6,5), (6,6), \\\n",
    "             (7,8), (8,4), (8,8)]\n",
    "#this is a list containing all blue cells\n",
    "snacks = [(0,0), (0,1), (0,2), (0,7), (0,8), (1,8), \\\n",
    "         (3,2), (3,3), (3,4), (3,5), (4,2), \\\n",
    "         (7,1), (7,2), (7,3), (7,4), (7,5), (7,6), \\\n",
    "         (8,1), (8,2), (8,3), (8,5), (8,6)]\n",
    "#this is a list containing all goal states\n",
    "terminalStates = [(3,8)]\n",
    "#this is an array containing all actions\n",
    "actions = np.array([0, 1, 2, 3, 4, 5, 6, 7]) #[NW,      N,      NE,     E,     SE,    S,     SW,     W]\n",
    "#example of how to unpack getNextStatesRewardsAndProbabilities(state, action):\n",
    "#create dummy state and action\n",
    "s_test = (0,6)\n",
    "a_test = 3\n",
    "#call helper function and loop over the return values\n",
    "for sPrime, R, P in getNextStatesRewardsAndProbabilities(state=s_test, action=a_test):\n",
    "    print('sPrime:', sPrime, 'R:', R, 'P:', P)\n",
    "    \n",
    "#once you have state values V, you can print them with okay'ish formatting like so:\n",
    "#print(\"State Values:\")\n",
    "#print(np.around(V, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2.3)\n",
    "\n",
    "Now it is time to find a good policy. Use the *Policy Iteration* algorithm to compute the optimal value $V^*(s)$ for each accessible cell.\n",
    "\n",
    "Retrieve the resulting optimal-policy $\\pi^*(s)$. To obtain a greedy policy given $V(s)$, make use of:\n",
    "\n",
    "$$\\pi_{greedy}(s) := \\operatorname{argmax}_a Q(s,a) = \\operatorname{argmax}_a \\sum_{s'}P_{ss'}^a\\cdot[R_{ss'}^a+\\gamma\\cdot V(s')]$$\n",
    "\n",
    "As implied by these terms, we recommend using intermediate state-action $Q$-values, shaped $9 \\times 9 \\times 8$ for this step!\n",
    "\n",
    "Finally, present your results by printing $V^*(s)$ and using our helper function *drawPolicy()* to visualize $\\pi^*(s,a)$.\n",
    "\n",
    "<div style=\"text-align: right; font-weight:bold\"> 5 Points </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.utils import drawPolicy\n",
    "#show policy using helper function as below\n",
    "#usage of the helper function, where pi is a (9,9,8) numpy array representing a deterministic policy:\n",
    "#drawPolicy(pi)\n",
    "#deterministic here means that one action per state has probability 1 and all others have probability 0\n",
    "#this will plot arrows representing your policies into the grid world."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2.4)\n",
    "\n",
    "Verify your results from the previous task by using the *Value Iteration* algorithm to compute the optimal value $V^*(s)$ for each cell. Make sure to reinitialize $V(s)$ with zero.\n",
    "\n",
    "Finally, present your results by printing $V^*(s)$ and using our helper function *drawPolicy()* to visualize $\\pi^*(s,a)$.\n",
    "\n",
    "<div style=\"text-align: right; font-weight:bold\"> 4 Points </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2.5)\n",
    "\n",
    "Modify your implementation of *Value Iteration* or *Policy Iteration* to ignore the random deviations in the environment. This can be achieved by calling *getNextStatesRewardsAndProbabilities(state, action, deviation=False)*.\n",
    "\n",
    "Present your results by printing $V^*(s)$ and using our helper function *drawPolicy()* to visualize $\\pi^*(s,a)$. How and why have your state values and policy changed?\n",
    "\n",
    "<div style=\"text-align: right; font-weight:bold\"> 3 Points </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please answer in this text cell."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
